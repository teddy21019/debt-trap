The approximated equilibrium is obtained by conducting value function iteration(VFI) over an $n_y \times n_d$ discretized and equally spaced state space, where $n_y = $ 200 is the number of grids for output process and $n_d=$ 200 is the number of grid for debt \citep{Na-18}. Denote $[\underline{y}^T, \overline{y}^T]$ as the lower and upper bound of output grid. Following \citet{Uribe-Schmitt-Grohe-textbook}, this is set as $[-4.2 \sigma_u, 4.2 \sigma_u]$. Also following the authors, since the average debt levels for both countries do not exceed 150\%, the upper bound for debt is set as 1.5, therefore the debt range for the VFI is $[\underline{d}, \overline{d}]=[0,1.5]$.

Due to the continuous assumption of the AR(1) process of $y^T_t$ (since we assume $\mu_t$ to be normal), the discretized method used in VFI is not directly applicable. \citet{Schmitt-Uribe-16} and \citet{Na-18} deal with this issue by constructing a transition probability matrix over the grids of the AR(1) output process.
A time series of 10 million observations was generated based on \refeq{eq:ar1-output}. Each observation was then assigned to the nearest grid point among the 200 discrete values of $\ln y^T$. The discretized series was analyzed to calculate the probabilities of transitioning from one discrete state to another in consecutive periods.
To obtain the transition probability matrix, a $200\times200$ matrix was initialized with zeros. For each pair of consecutive observations, the corresponding element in the matrix was incremented by 1. After considering all the observations, the matrix was normalized by dividing each row by the sum of its elements. This resulted in the estimated transition probability matrix, which effectively captured the covariance matrices of order 0 and 1.
\citep*{Uribe-Schmitt-Grohe-textbook}.


%%Value function Interation?
The equilibrium dynamics can be simulated once the VFI is conducted. Following \citet{Schmitt-Uribe-16}
and \citet{Na-18}, a simulation of based on the policy function is conduct 1.1 million time. After discarding the first 0.1 million periods, the periods in which a default occurs are identified, and a window of 12 quarters prior to and 12 quarters after each default episode is extracted. The median is then computed period by period across all windows, and the period of default is normalized to 0.